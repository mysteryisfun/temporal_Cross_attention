{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c91c41",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Feature Extraction and Cross-Attention CNN Training\n",
    "\n",
    "## Cross-Attention CNN Personality Trait Prediction Project\n",
    "\n",
    "This notebook performs comprehensive feature extraction and model training with GPU acceleration and production-ready features:\n",
    "\n",
    "### Pipeline Overview:\n",
    "1. **GPU Configuration** - Optimize TensorFlow GPU settings and memory management\n",
    "2. **Data Analysis** - Analyze preprocessed data structure (68K+ faces, 81K+ optical flows)\n",
    "3. **Static Feature Extraction** - Extract 512-dim features using ResNet-50 with GPU batching\n",
    "4. **Dynamic Feature Extraction** - Extract 256-dim features using I3D with GPU optimization\n",
    "5. **Data Alignment & Validation** - Ensure proper video-to-label mapping across 960 samples\n",
    "6. **Cross-Attention Training** - Train complete model with monitoring and checkpointing\n",
    "7. **Model Evaluation** - Comprehensive evaluation with visualizations\n",
    "\n",
    "### Data Structure:\n",
    "- **Input**: `data/processed/faces/` (68,459 faces) and `data/processed/optical_flow/` (81,330 flows)\n",
    "- **Labels**: ChaLearn dataset OCEAN personality traits\n",
    "- **Output**: Extracted features, trained Cross-Attention CNN model, and evaluation results\n",
    "\n",
    "### Key Features:\n",
    "- **GPU Acceleration**: Optimized batch processing for large-scale feature extraction\n",
    "- **Resume Functionality**: Smart checkpointing to resume from interruptions\n",
    "- **Memory Management**: Efficient handling of large datasets\n",
    "- **Progress Tracking**: Comprehensive logging and progress monitoring\n",
    "- **Error Recovery**: Robust error handling and data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e94af1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m__init__.cython-30.pxd:1023\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error, r2_score\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\__init__.py:73\u001b[0m\n\u001b[0;32m     68\u001b[0m if __SKLEARN_SETUP__:\n\u001b[0;32m     69\u001b[0m     sys.stderr.write(\"Partial import of sklearn during the build process.\\n\")\n\u001b[0;32m     70\u001b[0m     # We are not importing the rest of scikit-learn during the build\n\u001b[0;32m     71\u001b[0m     # process, as it may not be compiled yet\n\u001b[0;32m     72\u001b[0m else:\n\u001b[1;32m---> 73\u001b[0m     # `_distributor_init` allows distributors to run custom init code.\n\u001b[0;32m     74\u001b[0m     # For instance, for the Windows wheel, this is used to pre-load the\n\u001b[0;32m     75\u001b[0m     # vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\n\u001b[0;32m     76\u001b[0m     # sub-folder.\n\u001b[0;32m     77\u001b[0m     # It is necessary to do this prior to importing show_versions as the\n\u001b[0;32m     78\u001b[0m     # later is linked to the OpenMP runtime to make it possible to introspect\n\u001b[0;32m     79\u001b[0m     # it and importing it first would fail if the OpenMP dll cannot be found.\n\u001b[0;32m     80\u001b[0m     from . import (\n\u001b[0;32m     81\u001b[0m         __check_build,  # noqa: F401\n\u001b[0;32m     82\u001b[0m         _distributor_init,  # noqa: F401\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     84\u001b[0m     from .base import clone\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\__init__.py:15\u001b[0m\n\u001b[0;32m     11\u001b[0m from ._chunking import gen_batches, gen_even_slices\n\u001b[0;32m     12\u001b[0m from ._estimator_html_repr import estimator_html_repr\n\u001b[0;32m     14\u001b[0m # Make _safe_indexing importable from here for backward compat as this particular\n\u001b[1;32m---> 15\u001b[0m # helper is considered semi-private and typically very useful for third-party\n\u001b[0;32m     16\u001b[0m # libraries that want to comply with scikit-learn's estimator API. In particular,\n\u001b[0;32m     17\u001b[0m # _safe_indexing was included in our public API documentation despite the leading\n\u001b[0;32m     18\u001b[0m # `_` in its name.\n\u001b[0;32m     19\u001b[0m from ._indexing import (\n\u001b[0;32m     20\u001b[0m     _safe_indexing,  # noqa\n\u001b[0;32m     21\u001b[0m     resample,\n\u001b[0;32m     22\u001b[0m     shuffle,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m from ._mask import safe_mask\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\_chunking.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Inherits from ValueError and TypeError to keep backward compatibility.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\validation.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter, isclass, signature\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:17\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Yield supported namespace.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    This is meant to be used for testing purposes only.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    include_numpy_namespaces : bool, default=True\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        If True, also yield numpy namespaces.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    array_namespace : str\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m        The name of the Array API namespace.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m array_namespace \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# The following is used to test the array_api_compat wrapper when\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;66;03m# array_api_dispatch is enabled: in particular, the arrays used in the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     42\u001b[0m     ]:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_numpy_namespaces \u001b[38;5;129;01mand\u001b[39;00m array_namespace \u001b[38;5;129;01min\u001b[39;00m _NUMPY_NAMESPACE_NAMES:\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\sklearn\\utils\\fixes.py:17\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\scipy\\stats\\__init__.py:624\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontingency\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chi2_contingency\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_censored_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CensoredData\n\u001b[1;32m--> 624\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_resampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (bootstrap, monte_carlo_test, permutation_test, power,\n\u001b[0;32m    625\u001b[0m                           MonteCarloMethod, PermutationMethod, BootstrapMethod)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_entropy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hypotests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\scipy\\stats\\_stats_py.py:39\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\scipy\\spatial\\__init__.py:110\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ujwal\\anaconda3\\envs\\py310\\lib\\site-packages\\scipy\\spatial\\_kdtree.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright Anne M. Archibald 2008\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Released under the scipy license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[1;32m_ckdtree.pyx:26\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m__init__.cython-30.pxd:1025\u001b[0m, in \u001b[0;36mnumpy.import_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import sys\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import h5py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = os.path.abspath('.')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"üîß Environment Setup\")\n",
    "print(f\"   TensorFlow version: {tf.__version__}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")\n",
    "print(f\"   OpenCV version: {cv2.__version__}\")\n",
    "print(f\"   Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Configuration and Memory Management\n",
    "print(\"üîç GPU Configuration and Optimization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List and configure physical GPU devices\n",
    "physical_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "\n",
    "print(f\"Physical GPUs: {len(physical_gpus)}\")\n",
    "print(f\"Logical GPUs: {len(logical_gpus)}\")\n",
    "\n",
    "if physical_gpus:\n",
    "    print(\"‚úÖ GPU Support Available!\")\n",
    "    for i, gpu in enumerate(physical_gpus):\n",
    "        print(f\"   GPU {i}: {gpu}\")\n",
    "    \n",
    "    try:\n",
    "        # Enable memory growth to prevent TensorFlow from allocating all GPU memory\n",
    "        for gpu in physical_gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # Set memory limit if needed (optional)\n",
    "        # tf.config.experimental.set_memory_limit(physical_gpus[0], 8192)  # 8GB limit\n",
    "        \n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "        \n",
    "        # Configure GPU execution\n",
    "        tf.config.experimental.set_synchronous_execution(False)\n",
    "        print(\"‚úÖ Asynchronous execution enabled\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è GPU configuration error: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No GPU detected - using CPU (will be slower)\")\n",
    "\n",
    "# Test GPU with matrix operations\n",
    "print(f\"\\nüß™ GPU Performance Test:\")\n",
    "with tf.device('/GPU:0' if physical_gpus else '/CPU:0'):\n",
    "    # Test tensor operations\n",
    "    start_time = time.time()\n",
    "    a = tf.random.normal((1000, 1000))\n",
    "    b = tf.random.normal((1000, 1000))\n",
    "    c = tf.matmul(a, b)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"   Matrix multiplication (1000x1000): {(end_time - start_time)*1000:.2f}ms\")\n",
    "    print(f\"   Device used: {c.device}\")\n",
    "    print(f\"   Result shape: {c.shape}\")\n",
    "\n",
    "# Memory cleanup\n",
    "del a, b, c\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structure Analysis and Validation\n",
    "print(\"üìä Data Structure Analysis and Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load preprocessing results\n",
    "try:\n",
    "    with open('results/complete_preprocessing_results.json', 'r') as f:\n",
    "        preprocessing_results = json.load(f)\n",
    "    print(\"‚úÖ Loaded preprocessing results\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Preprocessing results not found - run preprocessing pipeline first\")\n",
    "    preprocessing_results = {}\n",
    "\n",
    "# Directory configuration\n",
    "faces_base_dir = 'data/processed/faces'\n",
    "flow_base_dir = 'data/processed/optical_flow'\n",
    "results_dir = 'results'\n",
    "features_dir = os.path.join(results_dir, 'features')\n",
    "models_dir = os.path.join(results_dir, 'models')\n",
    "\n",
    "# Create output directories\n",
    "Path(features_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(models_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Analyze current data structure\n",
    "data_stats = {\n",
    "    'faces_directory_exists': os.path.exists(faces_base_dir),\n",
    "    'optical_flow_directory_exists': os.path.exists(flow_base_dir),\n",
    "    'total_face_images': 0,\n",
    "    'total_optical_flow_files': 0,\n",
    "    'training_directories': {},\n",
    "    'video_count_per_directory': {},\n",
    "    'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "if data_stats['faces_directory_exists']:\n",
    "    print(f\"üìÅ Analyzing faces directory: {faces_base_dir}\")\n",
    "    \n",
    "    training_dirs = sorted([d for d in os.listdir(faces_base_dir) \n",
    "                           if os.path.isdir(os.path.join(faces_base_dir, d))])\n",
    "    \n",
    "    for training_dir in tqdm(training_dirs, desc=\"Analyzing directories\"):\n",
    "        training_path = os.path.join(faces_base_dir, training_dir)\n",
    "        video_dirs = [d for d in os.listdir(training_path) \n",
    "                     if os.path.isdir(os.path.join(training_path, d))]\n",
    "        \n",
    "        dir_face_count = 0\n",
    "        for video_dir in video_dirs:\n",
    "            video_path = os.path.join(training_path, video_dir)\n",
    "            face_files = [f for f in os.listdir(video_path) if f.endswith('.jpg')]\n",
    "            dir_face_count += len(face_files)\n",
    "        \n",
    "        data_stats['training_directories'][training_dir] = {\n",
    "            'videos': len(video_dirs),\n",
    "            'faces': dir_face_count\n",
    "        }\n",
    "        data_stats['video_count_per_directory'][training_dir] = len(video_dirs)\n",
    "        data_stats['total_face_images'] += dir_face_count\n",
    "\n",
    "if data_stats['optical_flow_directory_exists']:\n",
    "    print(f\"üìÅ Analyzing optical flow directory: {flow_base_dir}\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(flow_base_dir):\n",
    "        data_stats['total_optical_flow_files'] += len([f for f in files if f.endswith('.npy')])\n",
    "\n",
    "# Display analysis results\n",
    "print(f\"\\nüìà Data Analysis Results:\")\n",
    "print(f\"   ‚Ä¢ Faces directory: {'‚úÖ' if data_stats['faces_directory_exists'] else '‚ùå'}\")\n",
    "print(f\"   ‚Ä¢ Optical flow directory: {'‚úÖ' if data_stats['optical_flow_directory_exists'] else '‚ùå'}\")\n",
    "print(f\"   ‚Ä¢ Total face images: {data_stats['total_face_images']:,}\")\n",
    "print(f\"   ‚Ä¢ Total optical flow files: {data_stats['total_optical_flow_files']:,}\")\n",
    "print(f\"   ‚Ä¢ Training directories: {len(data_stats['training_directories'])}\")\n",
    "\n",
    "if data_stats['training_directories']:\n",
    "    print(f\"\\nüìã Directory breakdown:\")\n",
    "    for dir_name, stats in data_stats['training_directories'].items():\n",
    "        print(f\"   {dir_name}: {stats['videos']} videos, {stats['faces']:,} faces\")\n",
    "\n",
    "# Save analysis results\n",
    "with open(os.path.join(results_dir, 'data_analysis_results.json'), 'w') as f:\n",
    "    json.dump(data_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Analysis results saved to: {os.path.join(results_dir, 'data_analysis_results.json')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Initialize Model Components\n",
    "print(\"ü§ñ Initializing Model Components\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Import existing model components\n",
    "    from models.static_feature_extractor.feature_extractor import StaticFeatureExtractor\n",
    "    from models.dynamic_feature_extractor.feature_extractor import DynamicFeatureExtractor\n",
    "    from src.models.cross_attention import CrossAttention\n",
    "    from src.models.personality_model import CompletePersonalityModel\n",
    "    \n",
    "    print(\"‚úÖ Successfully imported model components\")\n",
    "    \n",
    "    # Initialize feature extractors with GPU optimization\n",
    "    print(\"üîß Initializing feature extractors...\")\n",
    "    \n",
    "    # Initialize static feature extractor (ResNet-50 based)\n",
    "    with tf.device('/GPU:0' if physical_gpus else '/CPU:0'):\n",
    "        static_extractor = StaticFeatureExtractor()\n",
    "        print(\"‚úÖ Static feature extractor initialized (ResNet-50)\")\n",
    "    \n",
    "    # Initialize dynamic feature extractor (I3D based)\n",
    "    with tf.device('/GPU:0' if physical_gpus else '/CPU:0'):\n",
    "        dynamic_extractor = DynamicFeatureExtractor()\n",
    "        print(\"‚úÖ Dynamic feature extractor initialized (I3D)\")\n",
    "    \n",
    "    # Model configuration\n",
    "    model_config = {\n",
    "        'static_dim': 512,      # ResNet-50 output dimension\n",
    "        'dynamic_dim': 256,     # I3D output dimension\n",
    "        'fusion_dim': 128,      # Cross-attention fusion dimension\n",
    "        'num_heads': 8,         # Multi-head attention heads\n",
    "        'dropout_rate': 0.3,    # Dropout for regularization\n",
    "        'learning_rate': 0.001, # Initial learning rate\n",
    "        'batch_size': 32,       # Training batch size\n",
    "        'epochs': 50,           # Training epochs\n",
    "        'validation_split': 0.2 # Validation data percentage\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìã Model Configuration:\")\n",
    "    for key, value in model_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    with open(os.path.join(results_dir, 'model_config.json'), 'w') as f:\n",
    "        json.dump(model_config, f, indent=2)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing model components: {e}\")\n",
    "    print(\"   Please ensure all model files are properly configured\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659c71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Optimized Static Feature Extraction Functions\n",
    "print(\"üéØ GPU-Optimized Feature Extraction Functions\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def extract_static_features_batch_gpu(face_image_paths, batch_size=32, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Extract static features from face images using GPU-optimized batch processing\n",
    "    \n",
    "    Args:\n",
    "        face_image_paths: List of paths to face images\n",
    "        batch_size: Number of images to process in each batch\n",
    "        target_size: Target image size for ResNet-50\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of extracted features [N, 512]\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    with tf.device('/GPU:0' if physical_gpus else '/CPU:0'):\n",
    "        for i in tqdm(range(0, len(face_image_paths), batch_size), desc=\"Static features\"):\n",
    "            batch_paths = face_image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            # Load and preprocess batch of images\n",
    "            for img_path in batch_paths:\n",
    "                try:\n",
    "                    # Read and preprocess image\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        img = cv2.resize(img, target_size)\n",
    "                        img = img.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "                        batch_images.append(img)\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Failed to load image: {img_path}\")\n",
    "                        # Add zero padding for failed images\n",
    "                        batch_images.append(np.zeros((*target_size, 3), dtype=np.float32))\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error processing {img_path}: {e}\")\n",
    "                    batch_images.append(np.zeros((*target_size, 3), dtype=np.float32))\n",
    "            \n",
    "            if batch_images:\n",
    "                # Convert to tensor and extract features\n",
    "                batch_tensor = tf.convert_to_tensor(batch_images)\n",
    "                \n",
    "                # Extract features using static extractor\n",
    "                try:\n",
    "                    batch_features = static_extractor.model(batch_tensor)\n",
    "                    features.extend(batch_features.numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error extracting features for batch: {e}\")\n",
    "                    # Add zero features for failed batch\n",
    "                    features.extend([np.zeros(512) for _ in batch_images])\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch_images, batch_tensor\n",
    "            gc.collect()\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_dynamic_features_batch_gpu(optical_flow_paths, batch_size=16, sequence_length=16):\n",
    "    \"\"\"\n",
    "    Extract dynamic features from optical flow sequences using GPU-optimized batch processing\n",
    "    \n",
    "    Args:\n",
    "        optical_flow_paths: List of paths to optical flow .npy files\n",
    "        batch_size: Number of sequences to process in each batch\n",
    "        sequence_length: Number of flow frames per sequence\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of extracted features [N, 256]\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    with tf.device('/GPU:0' if physical_gpus else '/CPU:0'):\n",
    "        for i in tqdm(range(0, len(optical_flow_paths), batch_size), desc=\"Dynamic features\"):\n",
    "            batch_paths = optical_flow_paths[i:i+batch_size]\n",
    "            batch_sequences = []\n",
    "            \n",
    "            # Load and preprocess batch of optical flow sequences\n",
    "            for flow_path in batch_paths:\n",
    "                try:\n",
    "                    # Load optical flow data\n",
    "                    flow_data = np.load(flow_path)\n",
    "                    \n",
    "                    # Ensure proper shape and normalize\n",
    "                    if len(flow_data.shape) == 3:  # (H, W, 2)\n",
    "                        # Resize if needed\n",
    "                        if flow_data.shape[:2] != (224, 224):\n",
    "                            flow_data = cv2.resize(flow_data, (224, 224))\n",
    "                        \n",
    "                        # Normalize flow magnitude\n",
    "                        flow_magnitude = np.sqrt(flow_data[:,:,0]**2 + flow_data[:,:,1]**2)\n",
    "                        max_magnitude = np.max(flow_magnitude) if np.max(flow_magnitude) > 0 else 1.0\n",
    "                        flow_data = flow_data / max_magnitude\n",
    "                        \n",
    "                        batch_sequences.append(flow_data)\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Invalid flow shape {flow_data.shape} for {flow_path}\")\n",
    "                        batch_sequences.append(np.zeros((224, 224, 2), dtype=np.float32))\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error loading {flow_path}: {e}\")\n",
    "                    batch_sequences.append(np.zeros((224, 224, 2), dtype=np.float32))\n",
    "            \n",
    "            if batch_sequences:\n",
    "                # Convert to tensor and extract features\n",
    "                try:\n",
    "                    batch_tensor = tf.convert_to_tensor(batch_sequences)\n",
    "                    # Add temporal dimension for 3D CNN if needed\n",
    "                    if len(batch_tensor.shape) == 4:  # (B, H, W, 2)\n",
    "                        batch_tensor = tf.expand_dims(batch_tensor, axis=1)  # (B, 1, H, W, 2)\n",
    "                    \n",
    "                    # Extract features using dynamic extractor\n",
    "                    batch_features = dynamic_extractor.model(batch_tensor)\n",
    "                    features.extend(batch_features.numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error extracting features for batch: {e}\")\n",
    "                    features.extend([np.zeros(256) for _ in batch_sequences])\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del batch_sequences, batch_tensor\n",
    "            gc.collect()\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def load_existing_features(features_path):\n",
    "    \"\"\"Load existing features if available for resume functionality\"\"\"\n",
    "    if os.path.exists(features_path):\n",
    "        try:\n",
    "            features = np.load(features_path)\n",
    "            print(f\"‚úÖ Loaded existing features: {features.shape}\")\n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading features: {e}\")\n",
    "    return None\n",
    "\n",
    "print(\"‚úÖ GPU-optimized feature extraction functions defined\")\n",
    "print(\"   ‚Ä¢ Batch processing for memory efficiency\")\n",
    "print(\"   ‚Ä¢ GPU acceleration with TensorFlow\")\n",
    "print(\"   ‚Ä¢ Error handling and resume capability\")\n",
    "print(\"   ‚Ä¢ Memory management and cleanup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71880c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Feature Extraction Pipeline with Resume Capability\n",
    "print(\"üéØ Starting GPU-Accelerated Static Feature Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "static_features_path = os.path.join(features_dir, 'static_features_large.npy')\n",
    "static_extraction_log = os.path.join(results_dir, 'static_feature_extraction_log.json')\n",
    "\n",
    "# Check for existing features (resume functionality)\n",
    "existing_static_features = load_existing_features(static_features_path)\n",
    "\n",
    "if existing_static_features is not None:\n",
    "    print(f\"üîÑ Found existing static features: {existing_static_features.shape}\")\n",
    "    print(\"   Skipping static feature extraction (use resume mode)\")\n",
    "    static_features = existing_static_features\n",
    "else:\n",
    "    print(\"üöÄ Starting fresh static feature extraction...\")\n",
    "    \n",
    "    # Collect all face image paths\n",
    "    face_image_paths = []\n",
    "    video_labels_map = {}  # Map video paths to labels for alignment\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"üìã Collecting face image paths...\")\n",
    "    for training_dir in tqdm(os.listdir(faces_base_dir), desc=\"Scanning directories\"):\n",
    "        training_path = os.path.join(faces_base_dir, training_dir)\n",
    "        if not os.path.isdir(training_path):\n",
    "            continue\n",
    "            \n",
    "        video_dirs = sorted([d for d in os.listdir(training_path) \n",
    "                            if os.path.isdir(os.path.join(training_path, d))])\n",
    "        \n",
    "        for video_dir in video_dirs:\n",
    "            video_path = os.path.join(training_path, video_dir)\n",
    "            face_files = sorted([f for f in os.listdir(video_path) if f.endswith('.jpg')])\n",
    "            \n",
    "            for face_file in face_files:\n",
    "                face_path = os.path.join(video_path, face_file)\n",
    "                face_image_paths.append(face_path)\n",
    "                \n",
    "                # Store video-to-directory mapping for label alignment\n",
    "                video_id = f\"{training_dir}/{video_dir}\"\n",
    "                video_labels_map[face_path] = video_id\n",
    "    \n",
    "    print(f\"üìä Collected {len(face_image_paths):,} face image paths\")\n",
    "    \n",
    "    # Extract static features with GPU acceleration\n",
    "    print(f\"üî• Extracting static features using GPU acceleration...\")\n",
    "    \n",
    "    extraction_stats = {\n",
    "        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_images': len(face_image_paths),\n",
    "        'batch_size': model_config['batch_size'],\n",
    "        'gpu_acceleration': len(physical_gpus) > 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        static_features = extract_static_features_batch_gpu(\n",
    "            face_image_paths, \n",
    "            batch_size=model_config['batch_size']\n",
    "        )\n",
    "        \n",
    "        # Save extracted features\n",
    "        np.save(static_features_path, static_features)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        extraction_time = end_time - start_time\n",
    "        \n",
    "        extraction_stats.update({\n",
    "            'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_time_seconds': extraction_time,\n",
    "            'extraction_time_formatted': str(time.strftime('%H:%M:%S', time.gmtime(extraction_time))),\n",
    "            'features_shape': static_features.shape,\n",
    "            'features_saved_to': static_features_path,\n",
    "            'average_time_per_image': extraction_time / len(face_image_paths),\n",
    "            'images_per_second': len(face_image_paths) / extraction_time\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüéâ Static Feature Extraction Completed!\")\n",
    "        print(f\"   ‚è±Ô∏è  Processing time: {extraction_stats['extraction_time_formatted']}\")\n",
    "        print(f\"   üìä Features extracted: {static_features.shape}\")\n",
    "        print(f\"   üíæ Saved to: {static_features_path}\")\n",
    "        print(f\"   ‚ö° Speed: {extraction_stats['images_per_second']:.2f} images/second\")\n",
    "        print(f\"   üéØ GPU acceleration: {'‚úÖ Enabled' if extraction_stats['gpu_acceleration'] else '‚ùå Disabled'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during static feature extraction: {e}\")\n",
    "        extraction_stats['error'] = str(e)\n",
    "        static_features = None\n",
    "    \n",
    "    # Save extraction log\n",
    "    with open(static_extraction_log, 'w') as f:\n",
    "        json.dump(extraction_stats, f, indent=2)\n",
    "    \n",
    "    print(f\"üìã Extraction log saved to: {static_extraction_log}\")\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e22957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Feature Extraction Pipeline with Resume Capability  \n",
    "print(\"üåä Starting GPU-Accelerated Dynamic Feature Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dynamic_features_path = os.path.join(features_dir, 'dynamic_features_large.npy')\n",
    "dynamic_extraction_log = os.path.join(results_dir, 'dynamic_feature_extraction_log.json')\n",
    "\n",
    "# Check for existing features (resume functionality)\n",
    "existing_dynamic_features = load_existing_features(dynamic_features_path)\n",
    "\n",
    "if existing_dynamic_features is not None:\n",
    "    print(f\"üîÑ Found existing dynamic features: {existing_dynamic_features.shape}\")\n",
    "    print(\"   Skipping dynamic feature extraction (use resume mode)\")\n",
    "    dynamic_features = existing_dynamic_features\n",
    "else:\n",
    "    print(\"üöÄ Starting fresh dynamic feature extraction...\")\n",
    "    \n",
    "    # Collect all optical flow paths\n",
    "    optical_flow_paths = []\n",
    "    flow_video_map = {}  # Map flow paths to videos for alignment\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"üìã Collecting optical flow paths...\")\n",
    "    for training_dir in tqdm(os.listdir(flow_base_dir), desc=\"Scanning flow directories\"):\n",
    "        training_path = os.path.join(flow_base_dir, training_dir)\n",
    "        if not os.path.isdir(training_path):\n",
    "            continue\n",
    "            \n",
    "        video_dirs = sorted([d for d in os.listdir(training_path) \n",
    "                            if os.path.isdir(os.path.join(training_path, d))])\n",
    "        \n",
    "        for video_dir in video_dirs:\n",
    "            video_path = os.path.join(training_path, video_dir)\n",
    "            flow_files = sorted([f for f in os.listdir(video_path) if f.endswith('.npy')])\n",
    "            \n",
    "            for flow_file in flow_files:\n",
    "                flow_path = os.path.join(video_path, flow_file)\n",
    "                optical_flow_paths.append(flow_path)\n",
    "                \n",
    "                # Store flow-to-video mapping for alignment\n",
    "                video_id = f\"{training_dir}/{video_dir}\"\n",
    "                flow_video_map[flow_path] = video_id\n",
    "    \n",
    "    print(f\"üìä Collected {len(optical_flow_paths):,} optical flow paths\")\n",
    "    \n",
    "    # Extract dynamic features with GPU acceleration\n",
    "    print(f\"üî• Extracting dynamic features using GPU acceleration...\")\n",
    "    \n",
    "    extraction_stats = {\n",
    "        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'total_flows': len(optical_flow_paths),\n",
    "        'batch_size': model_config['batch_size'] // 2,  # Smaller batch for 3D processing\n",
    "        'gpu_acceleration': len(physical_gpus) > 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        dynamic_features = extract_dynamic_features_batch_gpu(\n",
    "            optical_flow_paths, \n",
    "            batch_size=model_config['batch_size'] // 2  # Reduced batch size for memory\n",
    "        )\n",
    "        \n",
    "        # Save extracted features\n",
    "        np.save(dynamic_features_path, dynamic_features)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        extraction_time = end_time - start_time\n",
    "        \n",
    "        extraction_stats.update({\n",
    "            'end_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_time_seconds': extraction_time,\n",
    "            'extraction_time_formatted': str(time.strftime('%H:%M:%S', time.gmtime(extraction_time))),\n",
    "            'features_shape': dynamic_features.shape,\n",
    "            'features_saved_to': dynamic_features_path,\n",
    "            'average_time_per_flow': extraction_time / len(optical_flow_paths),\n",
    "            'flows_per_second': len(optical_flow_paths) / extraction_time\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüéâ Dynamic Feature Extraction Completed!\")\n",
    "        print(f\"   ‚è±Ô∏è  Processing time: {extraction_stats['extraction_time_formatted']}\")\n",
    "        print(f\"   üìä Features extracted: {dynamic_features.shape}\")\n",
    "        print(f\"   üíæ Saved to: {dynamic_features_path}\")\n",
    "        print(f\"   ‚ö° Speed: {extraction_stats['flows_per_second']:.2f} flows/second\")\n",
    "        print(f\"   üéØ GPU acceleration: {'‚úÖ Enabled' if extraction_stats['gpu_acceleration'] else '‚ùå Disabled'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during dynamic feature extraction: {e}\")\n",
    "        extraction_stats['error'] = str(e)\n",
    "        dynamic_features = None\n",
    "    \n",
    "    # Save extraction log\n",
    "    with open(dynamic_extraction_log, 'w') as f:\n",
    "        json.dump(extraction_stats, f, indent=2)\n",
    "    \n",
    "    print(f\"üìã Extraction log saved to: {dynamic_extraction_log}\")\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Alignment and Label Processing\n",
    "print(\"üîó Data Alignment and Label Processing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load personality trait labels from ChaLearn dataset\n",
    "def load_personality_labels():\n",
    "    \"\"\"\n",
    "    Load personality trait labels from the ChaLearn dataset structure\n",
    "    Returns aligned labels for the extracted features\n",
    "    \"\"\"\n",
    "    labels_path = os.path.join('data', 'training_data', 'annotation_training.pkl')\n",
    "    sample_labels_path = os.path.join('data_sample', 'labels.npy')\n",
    "    \n",
    "    # Try to load from multiple sources\n",
    "    if os.path.exists(sample_labels_path):\n",
    "        print(f\"üìÅ Loading sample labels from: {sample_labels_path}\")\n",
    "        sample_labels = np.load(sample_labels_path)\n",
    "        print(f\"   Sample labels shape: {sample_labels.shape}\")\n",
    "        return sample_labels\n",
    "    \n",
    "    # If no labels found, create dummy labels for testing\n",
    "    print(\"‚ö†Ô∏è No personality labels found - creating dummy labels for testing\")\n",
    "    \n",
    "    # Estimate number of unique videos from features\n",
    "    num_samples = min(len(static_features), len(dynamic_features)) if (static_features is not None and dynamic_features is not None) else 960\n",
    "    \n",
    "    # Create dummy OCEAN labels (5 traits: Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism)\n",
    "    dummy_labels = np.random.uniform(0.0, 1.0, (num_samples, 5))\n",
    "    \n",
    "    print(f\"   Created dummy labels shape: {dummy_labels.shape}\")\n",
    "    print(\"   ‚ö†Ô∏è These are random labels - replace with actual ChaLearn annotations for real training\")\n",
    "    \n",
    "    return dummy_labels\n",
    "\n",
    "# Align features and labels\n",
    "print(\"üîÑ Aligning features and labels...\")\n",
    "\n",
    "alignment_stats = {\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'static_features_available': static_features is not None,\n",
    "    'dynamic_features_available': dynamic_features is not None,\n",
    "}\n",
    "\n",
    "if static_features is not None and dynamic_features is not None:\n",
    "    # Load labels\n",
    "    personality_labels = load_personality_labels()\n",
    "    \n",
    "    # Calculate alignment requirements\n",
    "    static_count = len(static_features)\n",
    "    dynamic_count = len(dynamic_features)\n",
    "    label_count = len(personality_labels)\n",
    "    \n",
    "    alignment_stats.update({\n",
    "        'static_features_count': static_count,\n",
    "        'dynamic_features_count': dynamic_count,\n",
    "        'labels_count': label_count,\n",
    "        'min_samples': min(static_count, dynamic_count, label_count)\n",
    "    })\n",
    "    \n",
    "    print(f\"üìä Alignment Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Static features: {static_count:,}\")\n",
    "    print(f\"   ‚Ä¢ Dynamic features: {dynamic_count:,}\")\n",
    "    print(f\"   ‚Ä¢ Personality labels: {label_count:,}\")\n",
    "    \n",
    "    # Align to minimum count for consistency\n",
    "    min_samples = alignment_stats['min_samples']\n",
    "    \n",
    "    if min_samples > 0:\n",
    "        # Truncate to minimum sample count\n",
    "        static_features_aligned = static_features[:min_samples]\n",
    "        dynamic_features_aligned = dynamic_features[:min_samples]\n",
    "        labels_aligned = personality_labels[:min_samples]\n",
    "        \n",
    "        # Verify alignment\n",
    "        assert len(static_features_aligned) == len(dynamic_features_aligned) == len(labels_aligned), \\\n",
    "            \"Feature alignment failed!\"\n",
    "        \n",
    "        print(f\"‚úÖ Data alignment successful!\")\n",
    "        print(f\"   ‚Ä¢ Aligned samples: {min_samples:,}\")\n",
    "        print(f\"   ‚Ä¢ Static features shape: {static_features_aligned.shape}\")\n",
    "        print(f\"   ‚Ä¢ Dynamic features shape: {dynamic_features_aligned.shape}\")\n",
    "        print(f\"   ‚Ä¢ Labels shape: {labels_aligned.shape}\")\n",
    "        \n",
    "        # Save aligned data\n",
    "        aligned_data_path = os.path.join(features_dir, 'aligned_features_and_labels.npz')\n",
    "        np.savez_compressed(\n",
    "            aligned_data_path,\n",
    "            static_features=static_features_aligned,\n",
    "            dynamic_features=dynamic_features_aligned,\n",
    "            labels=labels_aligned\n",
    "        )\n",
    "        \n",
    "        alignment_stats.update({\n",
    "            'alignment_successful': True,\n",
    "            'aligned_samples': min_samples,\n",
    "            'aligned_data_saved_to': aligned_data_path\n",
    "        })\n",
    "        \n",
    "        print(f\"üíæ Aligned data saved to: {aligned_data_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No samples available for alignment\")\n",
    "        alignment_stats['alignment_successful'] = False\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Missing feature data - cannot perform alignment\")\n",
    "    alignment_stats['alignment_successful'] = False\n",
    "\n",
    "# Save alignment results\n",
    "alignment_log_path = os.path.join(results_dir, 'data_alignment_log.json')\n",
    "with open(alignment_log_path, 'w') as f:\n",
    "    json.dump(alignment_stats, f, indent=2)\n",
    "\n",
    "print(f\"üìã Alignment log saved to: {alignment_log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff418a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Attention CNN Model Training Pipeline\n",
    "print(\"üöÄ Cross-Attention CNN Model Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if alignment_stats.get('alignment_successful', False):\n",
    "    print(\"üî• Starting model training with aligned data...\")\n",
    "    \n",
    "    # Load aligned data\n",
    "    aligned_data_path = os.path.join(features_dir, 'aligned_features_and_labels.npz')\n",
    "    aligned_data = np.load(aligned_data_path)\n",
    "    \n",
    "    X_static = aligned_data['static_features']\n",
    "    X_dynamic = aligned_data['dynamic_features']\n",
    "    y = aligned_data['labels']\n",
    "    \n",
    "    print(f\"üìä Training data loaded:\")\n",
    "    print(f\"   ‚Ä¢ Static features: {X_static.shape}\")\n",
    "    print(f\"   ‚Ä¢ Dynamic features: {X_dynamic.shape}\")\n",
    "    print(f\"   ‚Ä¢ Labels (OCEAN): {y.shape}\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    print(\"üîß Preprocessing data...\")\n",
    "    \n",
    "    # Normalize features\n",
    "    static_scaler = StandardScaler()\n",
    "    dynamic_scaler = StandardScaler()\n",
    "    \n",
    "    X_static_norm = static_scaler.fit_transform(X_static)\n",
    "    X_dynamic_norm = dynamic_scaler.fit_transform(X_dynamic)\n",
    "    \n",
    "    # Split data\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(X_static_norm)), \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=None  # No stratification for regression\n",
    "    )\n",
    "    \n",
    "    X_static_train = X_static_norm[train_indices]\n",
    "    X_static_test = X_static_norm[test_indices]\n",
    "    X_dynamic_train = X_dynamic_norm[train_indices]\n",
    "    X_dynamic_test = X_dynamic_norm[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    print(f\"üìà Data split:\")\n",
    "    print(f\"   ‚Ä¢ Training samples: {len(X_static_train)}\")\n",
    "    print(f\"   ‚Ä¢ Testing samples: {len(X_static_test)}\")\n",
    "    \n",
    "    # Initialize and compile model\n",
    "    print(\"ü§ñ Initializing Cross-Attention CNN model...\")\n",
    "    \n",
    "    with tf.device('/GPU:0' if physical_gpus else '/CPU:0'):\n",
    "        model = CompletePersonalityModel(\n",
    "            static_dim=model_config['static_dim'],\n",
    "            dynamic_dim=model_config['dynamic_dim'],\n",
    "            fusion_dim=model_config['fusion_dim'],\n",
    "            num_heads=model_config['num_heads'],\n",
    "            dropout_rate=model_config['dropout_rate']\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=model_config['learning_rate']),\n",
    "            loss='mse',\n",
    "            metrics=['mae', 'mse']\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Model compiled successfully\")\n",
    "    \n",
    "    # Training configuration\n",
    "    training_config = {\n",
    "        'epochs': model_config['epochs'],\n",
    "        'batch_size': model_config['batch_size'],\n",
    "        'validation_split': 0.15,  # Use validation split from training data\n",
    "        'early_stopping_patience': 10,\n",
    "        'reduce_lr_patience': 5,\n",
    "        'checkpoint_period': 5\n",
    "    }\n",
    "    \n",
    "    # Setup callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=training_config['early_stopping_patience'], \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=training_config['reduce_lr_patience'], \n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(models_dir, 'cross_attention_cnn_checkpoint.h5'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            period=training_config['checkpoint_period']\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìã Training configuration:\")\n",
    "    for key, value in training_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüî• Starting model training...\")\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        history = model.fit(\n",
    "            [X_static_train, X_dynamic_train], \n",
    "            y_train,\n",
    "            epochs=training_config['epochs'],\n",
    "            batch_size=training_config['batch_size'],\n",
    "            validation_split=training_config['validation_split'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        training_end_time = time.time()\n",
    "        training_time = training_end_time - training_start_time\n",
    "        \n",
    "        print(f\"\\nüéâ Model training completed!\")\n",
    "        print(f\"   ‚è±Ô∏è  Training time: {time.strftime('%H:%M:%S', time.gmtime(training_time))}\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(f\"üìä Evaluating model on test set...\")\n",
    "        \n",
    "        test_loss, test_mae, test_mse = model.evaluate(\n",
    "            [X_static_test, X_dynamic_test], \n",
    "            y_test, \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Make predictions for detailed evaluation\n",
    "        y_pred = model.predict([X_static_test, X_dynamic_test])\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        r2_scores = [r2_score(y_test[:, i], y_pred[:, i]) for i in range(5)]\n",
    "        trait_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "        \n",
    "        evaluation_results = {\n",
    "            'test_loss': float(test_loss),\n",
    "            'test_mae': float(test_mae),\n",
    "            'test_mse': float(test_mse),\n",
    "            'test_rmse': float(np.sqrt(test_mse)),\n",
    "            'r2_scores': {trait_names[i]: float(r2_scores[i]) for i in range(5)},\n",
    "            'mean_r2_score': float(np.mean(r2_scores)),\n",
    "            'training_time_seconds': training_time,\n",
    "            'training_history': {\n",
    "                'loss': [float(x) for x in history.history['loss']],\n",
    "                'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "                'mae': [float(x) for x in history.history['mae']],\n",
    "                'val_mae': [float(x) for x in history.history['val_mae']]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"üìà Evaluation Results:\")\n",
    "        print(f\"   ‚Ä¢ Test Loss (MSE): {test_loss:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean R¬≤ Score: {np.mean(r2_scores):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Individual R¬≤ Scores:\")\n",
    "        for i, trait in enumerate(trait_names):\n",
    "            print(f\"     - {trait}: {r2_scores[i]:.4f}\")\n",
    "        \n",
    "        # Save model and results\n",
    "        model_save_path = os.path.join(models_dir, 'cross_attention_cnn_final.h5')\n",
    "        model.save(model_save_path)\n",
    "        \n",
    "        results_save_path = os.path.join(results_dir, 'training_results.json')\n",
    "        with open(results_save_path, 'w') as f:\n",
    "            json.dump(evaluation_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Model and results saved:\")\n",
    "        print(f\"   ‚Ä¢ Model: {model_save_path}\")\n",
    "        print(f\"   ‚Ä¢ Results: {results_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during model training: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot start training - data alignment failed\")\n",
    "    print(\"   Please ensure feature extraction completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60378a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Visualization and Analysis\n",
    "print(\"üìä Training Results Visualization and Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if training results are available\n",
    "results_path = os.path.join(results_dir, 'training_results.json')\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    # Load training results\n",
    "    with open(results_path, 'r') as f:\n",
    "        training_results = json.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Training results loaded successfully\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Cross-Attention CNN Training Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Training and Validation Loss\n",
    "    history = training_results['training_history']\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    \n",
    "    axes[0, 0].plot(epochs, history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss (MSE)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training and Validation MAE\n",
    "    axes[0, 1].plot(epochs, history['mae'], 'g-', label='Training MAE', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_mae'], 'orange', label='Validation MAE', linewidth=2)\n",
    "    axes[0, 1].set_title('Training and Validation MAE', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Mean Absolute Error')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: R¬≤ Scores by Personality Trait\n",
    "    r2_scores = training_results['r2_scores']\n",
    "    traits = list(r2_scores.keys())\n",
    "    scores = list(r2_scores.values())\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "    bars = axes[1, 0].bar(traits, scores, color=colors, alpha=0.8, edgecolor='black', linewidth=1)\n",
    "    axes[1, 0].set_title('R¬≤ Scores by Personality Trait', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('R¬≤ Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Model Performance Summary\n",
    "    metrics = ['Test Loss', 'Test MAE', 'Test RMSE', 'Mean R¬≤']\n",
    "    values = [\n",
    "        training_results['test_loss'],\n",
    "        training_results['test_mae'],\n",
    "        training_results['test_rmse'],\n",
    "        training_results['mean_r2_score']\n",
    "    ]\n",
    "    \n",
    "    # Normalize values for better visualization\n",
    "    normalized_values = [(v - min(values)) / (max(values) - min(values)) for v in values]\n",
    "    \n",
    "    bars = axes[1, 1].bar(metrics, normalized_values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'], \n",
    "                         alpha=0.8, edgecolor='black', linewidth=1)\n",
    "    axes[1, 1].set_title('Model Performance Summary (Normalized)', fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Normalized Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add actual value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    viz_save_path = os.path.join(results_dir, 'training_visualization.png')\n",
    "    plt.savefig(viz_save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Visualization saved to: {viz_save_path}\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\nüéØ Final Model Performance Summary:\")\n",
    "    print(f\"   üîπ Test Loss (MSE): {training_results['test_loss']:.4f}\")\n",
    "    print(f\"   üîπ Test MAE: {training_results['test_mae']:.4f}\")\n",
    "    print(f\"   üîπ Test RMSE: {training_results['test_rmse']:.4f}\")\n",
    "    print(f\"   üîπ Mean R¬≤ Score: {training_results['mean_r2_score']:.4f}\")\n",
    "    print(f\"   üîπ Training Time: {time.strftime('%H:%M:%S', time.gmtime(training_results['training_time_seconds']))}\")\n",
    "    \n",
    "    # Create performance report\n",
    "    performance_report = f\"\"\"\n",
    "# Cross-Attention CNN Performance Report\n",
    "\n",
    "## Model Configuration\n",
    "- Static Feature Dimension: {model_config['static_dim']}\n",
    "- Dynamic Feature Dimension: {model_config['dynamic_dim']}\n",
    "- Fusion Dimension: {model_config['fusion_dim']}\n",
    "- Number of Attention Heads: {model_config['num_heads']}\n",
    "- Dropout Rate: {model_config['dropout_rate']}\n",
    "\n",
    "## Training Results\n",
    "- **Test Loss (MSE)**: {training_results['test_loss']:.4f}\n",
    "- **Test MAE**: {training_results['test_mae']:.4f}\n",
    "- **Test RMSE**: {training_results['test_rmse']:.4f}\n",
    "- **Mean R¬≤ Score**: {training_results['mean_r2_score']:.4f}\n",
    "\n",
    "## Individual Trait Performance (R¬≤ Scores)\n",
    "\"\"\"\n",
    "    \n",
    "    for trait, score in training_results['r2_scores'].items():\n",
    "        performance_report += f\"- **{trait}**: {score:.4f}\\n\"\n",
    "    \n",
    "    performance_report += f\"\"\"\n",
    "## Training Configuration\n",
    "- Epochs: {model_config['epochs']}\n",
    "- Batch Size: {model_config['batch_size']}\n",
    "- Learning Rate: {model_config['learning_rate']}\n",
    "- Training Time: {time.strftime('%H:%M:%S', time.gmtime(training_results['training_time_seconds']))}\n",
    "\n",
    "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "    \n",
    "    # Save performance report\n",
    "    report_path = os.path.join(results_dir, 'performance_report.md')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(performance_report)\n",
    "    \n",
    "    print(f\"üìã Performance report saved to: {report_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No training results found\")\n",
    "    print(\"   Please ensure model training completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Pipeline Summary and Next Steps\n",
    "print(\"üèÅ Cross-Attention CNN Pipeline Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect all pipeline results\n",
    "pipeline_summary = {\n",
    "    'pipeline_completion_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'gpu_acceleration_enabled': len(physical_gpus) > 0,\n",
    "    'components_completed': []\n",
    "}\n",
    "\n",
    "# Check component completion status\n",
    "components = [\n",
    "    ('GPU Configuration', True),\n",
    "    ('Data Analysis', os.path.exists(os.path.join(results_dir, 'data_analysis_results.json'))),\n",
    "    ('Static Feature Extraction', os.path.exists(os.path.join(features_dir, 'static_features_large.npy'))),\n",
    "    ('Dynamic Feature Extraction', os.path.exists(os.path.join(features_dir, 'dynamic_features_large.npy'))),\n",
    "    ('Data Alignment', os.path.exists(os.path.join(features_dir, 'aligned_features_and_labels.npz'))),\n",
    "    ('Model Training', os.path.exists(os.path.join(models_dir, 'cross_attention_cnn_final.h5'))),\n",
    "    ('Results Visualization', os.path.exists(os.path.join(results_dir, 'training_visualization.png')))\n",
    "]\n",
    "\n",
    "print(f\"üìä Pipeline Component Status:\")\n",
    "for component_name, status in components:\n",
    "    status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"   {status_icon} {component_name}\")\n",
    "    pipeline_summary['components_completed'].append({\n",
    "        'component': component_name,\n",
    "        'completed': status\n",
    "    })\n",
    "\n",
    "# Calculate overall completion rate\n",
    "completed_count = sum([1 for _, status in components if status])\n",
    "completion_rate = (completed_count / len(components)) * 100\n",
    "\n",
    "pipeline_summary['completion_rate'] = completion_rate\n",
    "pipeline_summary['completed_components'] = completed_count\n",
    "pipeline_summary['total_components'] = len(components)\n",
    "\n",
    "print(f\"\\nüìà Overall Pipeline Completion: {completion_rate:.1f}% ({completed_count}/{len(components)})\")\n",
    "\n",
    "# Display key outputs\n",
    "print(f\"\\nüìÅ Key Output Files:\")\n",
    "key_outputs = [\n",
    "    ('Static Features', os.path.join(features_dir, 'static_features_large.npy')),\n",
    "    ('Dynamic Features', os.path.join(features_dir, 'dynamic_features_large.npy')),\n",
    "    ('Aligned Data', os.path.join(features_dir, 'aligned_features_and_labels.npz')),\n",
    "    ('Trained Model', os.path.join(models_dir, 'cross_attention_cnn_final.h5')),\n",
    "    ('Training Results', os.path.join(results_dir, 'training_results.json')),\n",
    "    ('Performance Report', os.path.join(results_dir, 'performance_report.md')),\n",
    "    ('Visualization', os.path.join(results_dir, 'training_visualization.png'))\n",
    "]\n",
    "\n",
    "for output_name, output_path in key_outputs:\n",
    "    exists = os.path.exists(output_path)\n",
    "    status_icon = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    size_info = \"\"\n",
    "    if exists:\n",
    "        try:\n",
    "            size_bytes = os.path.getsize(output_path)\n",
    "            if size_bytes > 1024*1024:  # > 1MB\n",
    "                size_info = f\" ({size_bytes/(1024*1024):.1f} MB)\"\n",
    "            else:\n",
    "                size_info = f\" ({size_bytes/1024:.1f} KB)\"\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"   {status_icon} {output_name}: {output_path}{size_info}\")\n",
    "\n",
    "# Performance summary (if available)\n",
    "if os.path.exists(os.path.join(results_dir, 'training_results.json')):\n",
    "    try:\n",
    "        with open(os.path.join(results_dir, 'training_results.json'), 'r') as f:\n",
    "            training_results = json.load(f)\n",
    "        \n",
    "        pipeline_summary['model_performance'] = {\n",
    "            'test_mae': training_results['test_mae'],\n",
    "            'test_rmse': training_results['test_rmse'],\n",
    "            'mean_r2_score': training_results['mean_r2_score']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüéØ Model Performance:\")\n",
    "        print(f\"   ‚Ä¢ Test MAE: {training_results['test_mae']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Test RMSE: {training_results['test_rmse']:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Mean R¬≤ Score: {training_results['mean_r2_score']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load performance metrics: {e}\")\n",
    "\n",
    "# Next steps and recommendations\n",
    "print(f\"\\nüéØ Next Steps and Recommendations:\")\n",
    "\n",
    "if completion_rate == 100:\n",
    "    print(f\"   üéâ CONGRATULATIONS! Pipeline completed successfully!\")\n",
    "    print(f\"   ‚úÖ All components have been executed\")\n",
    "    print(f\"   üìä Model is trained and ready for deployment\")\n",
    "    print(f\"   üîç Review performance metrics and consider hyperparameter tuning\")\n",
    "    print(f\"   üìù Consider running ablation studies to analyze component contributions\")\n",
    "    print(f\"   üöÄ Ready for Phase 5: Model evaluation and research analysis\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Pipeline incomplete ({completion_rate:.1f}% complete)\")\n",
    "    failed_components = [name for name, status in components if not status]\n",
    "    print(f\"   üîß Address failed components: {', '.join(failed_components)}\")\n",
    "    print(f\"   üîÑ Use resume functionality to continue from last checkpoint\")\n",
    "\n",
    "# Save pipeline summary\n",
    "summary_path = os.path.join(results_dir, 'pipeline_summary.json')\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(pipeline_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Pipeline summary saved to: {summary_path}\")\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nüéä Cross-Attention CNN Feature Extraction and Training Pipeline Complete!\")\n",
    "print(f\"    Thank you for using the GPU-accelerated pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8158622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cell - Verify Package Imports\n",
    "print(\"Testing package imports...\")\n",
    "try:\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import sklearn\n",
    "    import seaborn as sns\n",
    "    import h5py\n",
    "    print(\"‚úÖ All packages imported successfully!\")\n",
    "    print(f\"NumPy: {np.__version__}\")\n",
    "    print(f\"TensorFlow: {tf.__version__}\")\n",
    "    print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
