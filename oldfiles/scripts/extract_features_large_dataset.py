"""
Large-Scale Feature Extraction Pipeline

This script extracts static and dynamic features from the preprocessed face images 
and optical flow sequences generated by the GPU face extraction notebook.

Input:
- data/processed/faces/ (68,459 face images from 800 videos)
- data/processed/optical_flow/ (81,330 optical flow sequences)

Output:
- data/features/static_features_large.npy (800 x 512)
- data/features/dynamic_features_large.npy (800 x 256)
- data/features/labels_large.npy (800 x 5)

Usage:
    if (Test-Path .\env\Scripts\activate.ps1) {
        .\env\Scripts\activate.ps1
        python scripts/extract_features_large_dataset.py --faces_dir data/processed/faces --flow_dir data/processed/optical_flow --output_dir data/features --batch_size 16
    }
"""

import os
import sys
import argparse
import numpy as np
import tensorflow as tf
import cv2
from pathlib import Path
from tqdm import tqdm
import json
from datetime import datetime
import time

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from utils.logger import get_logger
from models.static_feature_extractor.feature_extractor import StaticFeatureExtractor
from models.dynamic_feature_extractor.feature_extractor import DynamicFeatureExtractor


def parse_args():
    parser = argparse.ArgumentParser(description="Large-Scale Feature Extraction Pipeline")
    parser.add_argument('--faces_dir', type=str, default='data/processed/faces', 
                       help='Directory containing face images')
    parser.add_argument('--flow_dir', type=str, default='data/processed/optical_flow',
                       help='Directory containing optical flow sequences')
    parser.add_argument('--output_dir', type=str, default='data/features',
                       help='Directory to save extracted features')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size for feature extraction')
    parser.add_argument('--max_videos', type=int, default=None,
                       help='Maximum number of videos to process (for testing)')
    return parser.parse_args()


def get_video_directories(base_dir):
    """Get all video directories across training folders"""
    video_dirs = []
    
    if not os.path.exists(base_dir):
        return video_dirs
    
    # Get all training directories (training80_01, training80_02, etc.)
    training_dirs = sorted([d for d in os.listdir(base_dir) 
                           if os.path.isdir(os.path.join(base_dir, d)) and d.startswith('training80_')])
    
    for training_dir in training_dirs:
        training_path = os.path.join(base_dir, training_dir)
        # Get all video directories within this training directory
        videos = sorted([d for d in os.listdir(training_path)
                        if os.path.isdir(os.path.join(training_path, d))])
        
        for video_dir in videos:
            video_path = os.path.join(training_path, video_dir)
            video_dirs.append({
                'path': video_path,
                'training_dir': training_dir,
                'video_id': video_dir
            })
    
    return video_dirs


def extract_static_features_from_faces(faces_dir, extractor, batch_size=16):
    """Extract static features from face images in a directory"""
    
    # Get all face image files
    face_files = sorted([f for f in os.listdir(faces_dir) 
                        if f.endswith('.jpg') and f.startswith('face_')])
    
    if not face_files:
        return None
    
    # Load and preprocess face images
    face_images = []
    for face_file in face_files:
        face_path = os.path.join(faces_dir, face_file)
        img = cv2.imread(face_path)
        if img is not None:
            # Resize to expected input size (224x224)
            img = cv2.resize(img, (224, 224))
            # Convert BGR to RGB for model
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            # Normalize to [0,1]
            img = img.astype(np.float32) / 255.0
            face_images.append(img)
    
    if not face_images:
        return None
    
    # Convert to numpy array and add batch dimension if needed
    face_images = np.array(face_images)
    
    # Extract features using the static feature extractor
    try:
        # Average features across all faces in the video
        features = extractor.extract_features(face_images)
        # Return mean feature vector for this video
        return np.mean(features, axis=0)
    except Exception as e:
        print(f"Error extracting static features: {e}")
        return None


def extract_dynamic_features_from_flows(flow_dir, extractor, sequence_length=16):
    """Extract dynamic features from optical flow sequences in a directory"""
    
    # Get all optical flow files
    flow_files = sorted([f for f in os.listdir(flow_dir) 
                        if f.endswith('.npy') and f.startswith('flow_')])
    
    if len(flow_files) < sequence_length:
        return None
    
    # Load optical flow sequences
    flow_sequences = []
    for flow_file in flow_files[:sequence_length]:  # Take first 16 flows
        flow_path = os.path.join(flow_dir, flow_file)
        try:
            flow = np.load(flow_path)
            # Resize flow to expected input size (224x224x2)
            if flow.shape[:2] != (224, 224):
                flow = cv2.resize(flow, (224, 224))
            flow_sequences.append(flow)
        except Exception as e:
            print(f"Error loading flow {flow_file}: {e}")
            continue
    
    if len(flow_sequences) < sequence_length:
        return None
    
    # Convert to numpy array with shape (1, sequence_length, 224, 224, 2)
    flow_sequences = np.array(flow_sequences)
    flow_sequences = np.expand_dims(flow_sequences, axis=0)
    
    # Extract features using the dynamic feature extractor
    try:
        features = extractor.extract_features(flow_sequences)
        return features[0]  # Remove batch dimension
    except Exception as e:
        print(f"Error extracting dynamic features: {e}")
        return None


def load_annotation_labels(video_id):
    """Load OCEAN personality trait labels for a video (placeholder)"""
    # TODO: Implement actual label loading from annotation files
    # For now, return random labels as placeholder
    return np.random.rand(5)  # 5 OCEAN traits


def main():
    args = parse_args()
    logger = get_logger(experiment_name="large_scale_feature_extraction")
    
    print("ðŸš€ Starting Large-Scale Feature Extraction Pipeline")
    print("=" * 60)
    
    # Create output directory
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # Check GPU availability
    gpus = tf.config.list_physical_devices('GPU')
    print(f"GPU devices available: {len(gpus)}")
    
    # Initialize feature extractors
    print("ðŸ§  Initializing feature extractors...")
    static_extractor = StaticFeatureExtractor()
    dynamic_extractor = DynamicFeatureExtractor()
    print("âœ… Feature extractors initialized")
    
    # Get all video directories
    print("ðŸ“ Discovering video directories...")
    video_dirs = get_video_directories(args.faces_dir)
    
    if args.max_videos:
        video_dirs = video_dirs[:args.max_videos]
    
    print(f"Found {len(video_dirs)} video directories to process")
    
    if len(video_dirs) == 0:
        print("âŒ No video directories found!")
        return
    
    # Initialize arrays for features and labels
    static_features = []
    dynamic_features = []
    labels = []
    processed_videos = []
    failed_videos = []
    
    start_time = time.time()
    
    # Process each video directory
    print("\nðŸ”„ Processing videos...")
    for i, video_info in enumerate(tqdm(video_dirs, desc="Extracting features")):
        video_path = video_info['path']
        video_id = video_info['video_id']
        training_dir = video_info['training_dir']
        
        try:
            # Extract static features from faces
            faces_path = video_path  # Face images are in the video directory
            static_feat = extract_static_features_from_faces(faces_path, static_extractor, args.batch_size)
            
            # Extract dynamic features from optical flows
            flow_path = video_path.replace('/faces/', '/optical_flow/').replace('\\faces\\', '\\optical_flow\\')
            dynamic_feat = extract_dynamic_features_from_flows(flow_path, dynamic_extractor)
            
            # Load labels
            video_labels = load_annotation_labels(video_id)
            
            # Check if both features were extracted successfully
            if static_feat is not None and dynamic_feat is not None:
                static_features.append(static_feat)
                dynamic_features.append(dynamic_feat)
                labels.append(video_labels)
                processed_videos.append({
                    'video_id': video_id,
                    'training_dir': training_dir,
                    'static_shape': static_feat.shape,
                    'dynamic_shape': dynamic_feat.shape
                })
            else:
                failed_videos.append({
                    'video_id': video_id,
                    'training_dir': training_dir,
                    'static_success': static_feat is not None,
                    'dynamic_success': dynamic_feat is not None
                })
                
        except Exception as e:
            print(f"Error processing {video_id}: {e}")
            failed_videos.append({
                'video_id': video_id,
                'training_dir': training_dir,
                'error': str(e)
            })
            continue
        
        # Progress update every 50 videos
        if (i + 1) % 50 == 0:
            print(f"Processed {i + 1}/{len(video_dirs)} videos. "
                  f"Success: {len(processed_videos)}, Failed: {len(failed_videos)}")
    
    # Convert to numpy arrays
    if static_features and dynamic_features and labels:
        static_features = np.array(static_features)
        dynamic_features = np.array(dynamic_features)
        labels = np.array(labels)
        
        # Save features and labels
        static_path = os.path.join(args.output_dir, 'static_features_large.npy')
        dynamic_path = os.path.join(args.output_dir, 'dynamic_features_large.npy')
        labels_path = os.path.join(args.output_dir, 'labels_large.npy')
        
        np.save(static_path, static_features)
        np.save(dynamic_path, dynamic_features)
        np.save(labels_path, labels)
        
        print(f"\nâœ… Features successfully extracted and saved:")
        print(f"   ðŸ“Š Static features: {static_features.shape} â†’ {static_path}")
        print(f"   ðŸŒŠ Dynamic features: {dynamic_features.shape} â†’ {dynamic_path}")
        print(f"   ðŸ·ï¸  Labels: {labels.shape} â†’ {labels_path}")
        
        # Save processing results
        results = {
            'extraction_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'processing_time_seconds': time.time() - start_time,
            'total_videos_found': len(video_dirs),
            'successfully_processed': len(processed_videos),
            'failed_videos': len(failed_videos),
            'static_features_shape': static_features.shape,
            'dynamic_features_shape': dynamic_features.shape,
            'labels_shape': labels.shape,
            'processed_videos': processed_videos,
            'failed_videos': failed_videos
        }
        
        results_path = os.path.join(args.output_dir, 'extraction_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"   ðŸ“‹ Processing results: {results_path}")
        
    else:
        print("âŒ No features were successfully extracted!")
    
    processing_time = time.time() - start_time
    print(f"\nâ±ï¸ Total processing time: {processing_time:.1f} seconds")
    print(f"ðŸ“ˆ Success rate: {len(processed_videos)}/{len(video_dirs)} ({100*len(processed_videos)/len(video_dirs):.1f}%)")


if __name__ == "__main__":
    main()
