# Video Action Recognition with GASM-CAST: Graph-Aware Semantic-Motion Cross-Attention

## Project Overview

This research project presents a novel, efficient neural network architecture for video action recognition that introduces **GASM-CAST** (Graph-Aware Semantic-Motion Cross-Attention with Temporal Structure). It combines self-supervised vision transformers with 3D convolutional neural networks through an innovative graph-aware progressive fusion mechanism.

The core innovation involves fusing semantic features from **DINOv3 ViT-B/16** with motion features from **I3D R3D-18**, integrated through a sophisticated **3-layer progressive B-CAST** (Bottleneck Cross-Attention) system with intra-modal graph attention refinement.

**Current Status**: Phase 2 Complete âœ… | Phase 3 GASM-CAST Ready ðŸ”„ | Moving beyond simple cross-attention

## Research Problem & Motivation

### Limitations of Current Approaches
- **Simple cross-attention** lacks structural understanding of spatial/temporal relationships
- **Vanilla fusion** ignores intra-modal dependencies within semantic patches or motion segments  
- **Computational inefficiency** of full cross-modal attention (768D â†” 512D = expensive)
- **Single-layer learning** misses hierarchical correspondences from low-level to high-level patterns

### Our GASM-CAST Solution
We propose a graph-aware progressive architecture that:
1. **Graph Attention Networks**: Refine features within each modality using structural relationships
2. **Bottleneck Cross-Attention**: Efficient cross-modal fusion through compressed representation space
3. **Progressive Learning**: Hierarchical understanding from basic correspondences to action-level concepts
4. **Computational Efficiency**: 50% reduction in cross-attention computation while improving performance

## GASM-CAST Architecture

### Semantic Stream: DINOv3 ViT-B/16 with Graph Attention
- **Model**: Self-supervised Vision Transformer with 86M parameters  
- **Features**: 768-dimensional embeddings for 196 spatial patches (14Ã—14)
- **Graph Enhancement**: Learn which patches should attend to each other
- **Innovation**: Hand patches connect to object patches, background forms separate clusters
- **Output**: Context-aware 768D semantic features

### Motion Stream: I3D R3D-18 with Temporal Graph Attention  
- **Model**: 3D ResNet with Kinetics-400 pretraining
- **Features**: 512-dimensional features for 8 temporal segments
- **Graph Enhancement**: Learn which temporal segments should influence each other
- **Innovation**: Motion sequences with similar patterns become connected
- **Output**: Context-aware 512D motion features

### Progressive B-CAST Fusion Mechanism

#### Three-Layer Hierarchical Learning


#### Key Innovation: Learned Feature Interaction
- **Multi-head attention** (8 heads) enables diverse feature interactions
- **Bidirectional querying** allows both streams to inform each other
- **Residual connections** preserve original feature information
- **Layer normalization** stabilizes training dynamics

### Architecture Flow
1. **Input**: Video clip â†’ Uniformly sample 8-16 frames
2. **Semantic Processing**: Frames â†’ DINOv3 â†’ [frames, 1024] semantic features
3. **Motion Processing**: Frame stack â†’ I3D â†’ [temporal_segments, 2048] motion features
4. **Projection**: Both streams â†’ Common embedding space (512D)
5. **Cross-Attention**: Bidirectional attention fusion
6. **Classification**: Fused features â†’ 174 action categories

## Dataset: Something-Something-V2

### Dataset Characteristics
- **Scale**: 220,847 video clips
  - Training: 168,913 videos
  - Validation: 24,777 videos  
  - Test: 27,157 videos
- **Duration**: 2-6 seconds per clip (~60-180 frames)
- **Classes**: 174 action categories

### Unique Challenges
1. **Template-based Labels**: Actions described as "Pulling [something]" or "Holding [something] next to [something]"
2. **Temporal Focus**: Emphasizes motion patterns over static appearance
3. **Object Interaction**: Requires understanding of object relationships and manipulation
4. **Generalization**: Same action performed with different objects

### Why Something-Something-V2 is Ideal for Our Approach
- **Semantic Understanding**: Object recognition crucial for template completion
- **Temporal Dynamics**: Motion patterns essential for action discrimination  
- **Cross-modal Reasoning**: Requires integration of "what" (objects) and "how" (motion)
- **Benchmarking**: Established dataset for fair comparison with existing methods

## Technical Innovation & Novelty

### Core Contributions
1. **First DINOv3 + 3D CNN Fusion**: Novel combination of state-of-the-art self-supervised ViT with 3D CNN
2. **Cross-Attention Mechanism**: Sophisticated feature interaction beyond simple concatenation
3. **Parameter Efficiency**: 85%+ frozen parameters while maintaining high performance
4. **Interpretability**: Attention weights provide insights into semantic-motion interactions

### Comparison with Existing Approaches
| Method | Semantic Features | Motion Features | Fusion | Trainable % |
|--------|------------------|-----------------|---------|-------------|
| CAST | Hand-crafted | 3D CNN | Concatenation | 100% |
| TimeSformer | ViT | Self-attention | Temporal modeling | 100% |
| **Ours** | **DINOv2** | **I3D** | **Cross-attention** | **15%** |

## Expected Outcomes & Performance Targets

### Primary Metrics
- **Top-1 Accuracy**: 75-78% on Something-Something-V2 validation
- **Top-5 Accuracy**: 92-95% on Something-Something-V2 validation
- **Parameter Efficiency**: <20M trainable parameters vs 115M total

### Comparative Performance
- **Baseline CAST**: ~69% Top-1 accuracy[1]
- **Our Approach**: Expected 75-78% Top-1 accuracy (6-9% improvement)
- **Efficiency Gain**: 95% parameter reduction vs full fine-tuning

### Ablation Studies Planned
1. **Semantic Stream**: DINOv2 ViT-S/14 vs CLIP vs supervised pretraining
2. **Motion Stream**: I3D vs R(2+1)D vs C3D  
3. **Fusion Method**: Cross-attention vs concatenation vs element-wise
4. **Attention Heads**: 4 vs 8 vs 16 heads analysis

## Research Impact & Publications

### Publication Potential
- **Venue Target**: CVPR/ICCV/ECCV (top-tier computer vision conferences)
- **Novel Contributions**: Parameter-efficient fusion, DINOv3 integration, cross-attention mechanism
- **Practical Impact**: Enables high-performance video understanding on limited compute resources

### Broader Applications
- **Video Surveillance**: Action recognition in security systems
- **Sports Analysis**: Automated action detection and analysis
- **Human-Computer Interaction**: Gesture and activity recognition
- **Content Moderation**: Automated video content analysis

## Technical Advantages

### Computational Efficiency
- **Memory Usage**: Reduced through feature caching and frozen backbones
- **Training Time**: Faster convergence with focused parameter updates
- **Inference Speed**: Real-time capable on modern GPUs
- **Scalability**: Framework adaptable to other backbone models

### Interpretability Benefits
- **Attention Visualization**: Understand which motion patterns attend to which objects
- **Error Analysis**: Identify failure modes through attention patterns
- **Feature Quality Assessment**: Validate learned representations through visualization
- **Model Debugging**: Attention weights help diagnose training issues

This research combines cutting-edge self-supervised learning with established 3D CNN architectures to create a novel, efficient, and interpretable solution for video action recognition. The approach addresses current limitations while maintaining practical applicability for real-world deployment.
