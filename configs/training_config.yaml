# Training Configuration
experiment_name: "temporal_cross_attention_baseline"

# Dataset
dataset:
  name: "something_something_v2"
  train_split: "train"
  val_split: "validation"
  test_split: "test"

# Data Loading
data_loader:
  batch_size: 8
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2

# Training Parameters
training:
  max_epochs: 50
  accelerator: "gpu"
  devices: 1
  precision: "16-mixed"  # Mixed precision training

  # Gradient accumulation for larger effective batch size
  accumulate_grad_batches: 2

  # Validation
  check_val_every_n_epoch: 1
  val_check_interval: 1.0

  # Checkpointing
  enable_checkpointing: true
  save_top_k: 3
  monitor: "val_top1_accuracy"
  mode: "max"

# Optimizer
optimizer:
  name: "AdamW"
  lr: 0.0001
  weight_decay: 0.0001
  betas: [0.9, 0.999]

# Learning Rate Scheduler
scheduler:
  name: "CosineAnnealingLR"
  warmup_epochs: 5
  warmup_start_lr: 0.00001
  eta_min: 0.000001

# Loss Function
loss:
  name: "CrossEntropyLoss"
  label_smoothing: 0.1

# Early Stopping
early_stopping:
  monitor: "val_top1_accuracy"
  patience: 10
  mode: "max"

# Logging
logger:
  name: "tensorboard"
  save_dir: "experiments/logs"
  version: null

# Callbacks
callbacks:
  - name: "ModelCheckpoint"
    dirpath: "experiments/checkpoints"
    filename: "{epoch:02d}-{val_top1_accuracy:.2f}"
    monitor: "val_top1_accuracy"
    mode: "max"
    save_top_k: 3

  - name: "EarlyStopping"
    monitor: "val_top1_accuracy"
    patience: 10
    mode: "max"

  - name: "LearningRateMonitor"
    logging_interval: "epoch"

# Feature Caching
feature_cache:
  enable: true
  cache_dir: "data/features"
  overwrite: false

# Debug Mode
debug: false
fast_dev_run: false  # Set to true for quick testing
